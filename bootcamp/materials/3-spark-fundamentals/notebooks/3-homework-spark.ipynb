{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed11712-2bb3-4787-a3d1-b6d1d193a7dd",
   "metadata": {},
   "source": [
    "## SPARK homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18cc107-4431-4aef-aa1a-ad144404ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 16:36:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b12c2239318b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ba4a61760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, split, lit, avg, sum, desc, count, col\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Jupyter\")\n",
    "    # Q1. Disabled automatic broadcast join with `spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")`\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f6ba48-ff95-462e-9e5a-5fd1b5e5a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "matches = spark.read.csv(\"/home/iceberg/data/matches.csv\", header=True, inferSchema=True)\n",
    "match_details = spark.read.csv(\"/home/iceberg/data/match_details.csv\", header=True, inferSchema=True)\n",
    "medals_matches_players = spark.read.csv(\"/home/iceberg/data/medals_matches_players.csv\", header=True, inferSchema=True)\n",
    "medals = spark.read.csv(\"/home/iceberg/data/medals.csv\", header=True, inferSchema=True)\n",
    "maps = spark.read.csv(\"/home/iceberg/data/maps.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f264db-808f-4d63-8979-3b344fe053c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Explicitly broadcast JOINs `medals` and `maps`\n",
    "#   and rename some columns for later analysis\n",
    "\n",
    "medals_broadcast = broadcast(medals.withColumnRenamed(\"name\", \"medalname\"))\n",
    "maps_broadcast = broadcast(maps.withColumnRenamed(\"name\", \"mapname\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0bcc93-bf1b-4876-ba6a-3746f5217aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 16:37:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create the 4 bucketed tables\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS bootcamp.match_details_bucketed\")\n",
    "match_details.write.bucketBy(16, \"match_id\").mode(\"overwrite\").saveAsTable(\"bootcamp.match_details_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS bootcamp.matches_bucketed\")\n",
    "matches.write.bucketBy(16, \"match_id\").mode(\"overwrite\").saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS bootcamp.medals_matches_players_bucketed\")\n",
    "medals_matches_players.write.bucketBy(16, \"match_id\").mode(\"overwrite\").saveAsTable(\"bootcamp.medals_matches_players_bucketed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e065eddb-4996-4bb6-a9b3-c8e0143f6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the bucketed tables\n",
    "\n",
    "match_details_bucketed = spark.read.table(\"bootcamp.match_details_bucketed\")\n",
    "matches_bucketed = spark.read.table(\"bootcamp.matches_bucketed\")\n",
    "medals_matches_players_bucketed = spark.read.table(\"bootcamp.medals_matches_players_bucketed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7942794d-0f84-4f24-be3b-bf43a7697a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Bucket join `match_details`, `matches`, and `medal_matches_players` on `match_id` with `16` buckets\n",
    "#   with the broadcast data too\n",
    "analytical_df = match_details_bucketed \\\n",
    "    .join(matches_bucketed, \"match_id\", \"inner\") \\\n",
    "    .join(medals_matches_players_bucketed, [\"match_id\", \"player_gamertag\"], \"inner\") \\\n",
    "    .join(medals_broadcast, \"medal_id\", \"inner\") \\\n",
    "    .join(maps_broadcast, \"mapid\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6452fbfe-2068-4135-9e5b-61c83fb33732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(player_gamertag='gimpinator14', metric_player_avg_kills=109.0),\n",
       " Row(player_gamertag='I Johann117 I', metric_player_avg_kills=96.0),\n",
       " Row(player_gamertag='BudgetLegendary', metric_player_avg_kills=83.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.1. Player best average kills\n",
    "\n",
    "analytical_df \\\n",
    "    .groupBy(\"player_gamertag\") \\\n",
    "    .agg(avg(\"player_total_kills\").alias(\"metric_player_avg_kills\")) \\\n",
    "    .orderBy(desc(\"metric_player_avg_kills\")) \\\n",
    "    .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29fa1b2-0933-4280-9463-c73ca5886bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(playlist_id='f72e0ef0-7c4a-4307-af78-8e38dac3fdba', metric_playlist_plays=202489),\n",
       " Row(playlist_id='c98949ae-60a8-43dc-85d7-0feb0b92e719', metric_playlist_plays=107422),\n",
       " Row(playlist_id='2323b76a-db98-4e03-aa37-e171cfbdd1a4', metric_playlist_plays=92148)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.2. Playlist most played\n",
    "\n",
    "analytical_df \\\n",
    "    .groupBy(\"playlist_id\") \\\n",
    "    .agg(count(\"match_id\").alias(\"metric_playlist_plays\")) \\\n",
    "    .orderBy(desc(\"metric_playlist_plays\")) \\\n",
    "    .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05aded3f-3b1e-444d-8539-3be6bbc666b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(mapname='Breakout Arena', metric_map_plays=186118),\n",
       " Row(mapname='Alpine', metric_map_plays=105658),\n",
       " Row(mapname='Glacier', metric_map_plays=70182)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.3. Map most played\n",
    "\n",
    "analytical_df \\\n",
    "    .groupBy(\"mapname\") \\\n",
    "    .agg(count(\"match_id\").alias(\"metric_map_plays\")) \\\n",
    "    .orderBy(desc(\"metric_map_plays\")) \\\n",
    "    .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1832f1-2058-4e89-a98c-6f0ed10acb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(medalname='Killing Spree', mapname='Breakout Arena', metric_map_plays=6553),\n",
       " Row(medalname='Killing Spree', mapname='Alpine', metric_map_plays=4317),\n",
       " Row(medalname='Killing Spree', mapname='Glacier', metric_map_plays=2611)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.4. Most common maps played when medal \"Killing Spree\" is earned\n",
    "\n",
    "analytical_df \\\n",
    "    .filter(col(\"medalname\")==\"Killing Spree\") \\\n",
    "    .groupBy(\"medalname\", \"mapname\") \\\n",
    "    .agg(count(\"mapname\").alias(\"metric_map_plays\")) \\\n",
    "    .orderBy(desc(\"metric_map_plays\")) \\\n",
    "    .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58959321-ff4d-496b-a5b8-5dfc42187318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|   size|num_files|partitioned_by|\n",
      "+-------+---------+--------------+\n",
      "|3128095|        4|   playlist_id|\n",
      "+-------+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|   size|num_files|partitioned_by|\n",
      "+-------+---------+--------------+\n",
      "|3104734|        4|         mapid|\n",
      "+-------+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|   size|num_files|partitioned_by|\n",
      "+-------+---------+--------------+\n",
      "|2737615|        4|      match_id|\n",
      "+-------+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------+\n",
      "|   size|num_files| partitioned_by|\n",
      "+-------+---------+---------------+\n",
      "|4304312|        4|player_gamertag|\n",
      "+-------+---------+---------------+\n",
      "\n",
      "Size looks smaller with partition on match_id\n"
     ]
    }
   ],
   "source": [
    "# Q5. Get size when repartitionning of different columns\n",
    "\n",
    "for partition_column in [\"playlist_id\", \"mapid\", \"match_id\", \"player_gamertag\"]:\n",
    "    (\n",
    "        analytical_df\n",
    "        .repartition(4, partition_column)\n",
    "        .sortWithinPartitions(partition_column)\n",
    "        # Select only some cols beacause on join, several columns have same name\n",
    "        .select(\"medalname\", \"mapid\", \"mapname\", \"playlist_id\", \"match_id\", \"player_gamertag\")\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"bootcamp.analytical_df_partitionned_test\")\n",
    "    )\n",
    "    # spark.sql(\"Select * FROM demo.bootcamp.analytical_df_partitionned_test limit 5\").show()\n",
    "    spark.sql(\"DROP TABLE IF EXISTS demo.bootcamp.analytical_df_partitionned_test.files\")\n",
    "    spark.sql(\"SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, '\"+partition_column+\"' as partitioned_by FROM demo.bootcamp.analytical_df_partitionned_test.files\").show()\n",
    "\n",
    "print(\"Size looks smaller with partition on match_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169e930-8891-4eff-8483-2dcc5371c9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
